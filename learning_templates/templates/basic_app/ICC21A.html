{% load static %}
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICC (2,1,A)</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body style = "padding-left:2%;">
    
<h2>ICC(2,1,A)</h2>
<h4>- Defined by Shrout and Fleiss (1979) and McGraw and Wong (1996).</h4>
<h4>- Calculate the degree of absolute agreement among single measurements rated by randomly selected raters.</h4>

<h3>Interpretation:</h3>

Degree of absolute agreement among single measurements rated by randomly selected raters. 
Note that this has the same formula as ICC(1,A), however the assumption is assuming raters are randomly selected as opposed to a specific set of raters.
<br>
Koo and Li (2016) gives the following suggestion for interpreting ICC (Koo and Li 2016):
<br>
below 0.50: Poor<br>
between 0.50 and 0.75: Moderate<br>
between 0.75 and 0.90: Good<br>
above 0.90: Excellent<br>

<h3>Underlying Model: Two-way random effects model</h3>

<p>\(x_{ij} = \mu + r_i + c_j + e_{ij} \)</p>

<h3>Assumptions:</h3>
<p>\(\mu\): The population mean of observations.</p>
<p>\(r_i\): The row effects (effects between each sample) are random, independent and normally distributed with mean 0 and variance \(\sigma_{r}^2\).</p>
<p>\(c_i\): The row effects (effects between each rater) are random, independent and normally distributed with mean 0 and variance \(\sigma_{c}^2\).</p>
<p>\(e_{ij}\): The residual error are random, independent and normally distributed with mean 0 and variance \(\sigma_{e}^2\). All residual effects are pairwise independent.</p>

<h3>Formula:</h3>

<p>\(\frac{MS_R - MS_E}{MS_R + (k-1) MS_E + \frac{k}{n}(MS_c - MS_E)}\)</p>

<p>\(MS_R\) = mean square for rows; </p>
<p> \(MS_C\) = mean square for columns; </p>
<p> \(MS_E\) = mean square error; </p>
  <p> k = number of measurements (number of columns); </p>
 <p> n = number of object of measurement (number of rows);</p>

 <h3>Example in R</h3>

 <div style = "width: 40%;">
 {% include "basic_app/icc21a_r.html" %}

 </div>



 <h3>Live Example: Try this yourself</h3>

<iframe width='40%' height='40%' src='https://rdrr.io/snippets/embed/?code=data(%22anxiety%22%2C%20package%20%3D%20%22irr%22)%0Ahead(anxiety%2C%204)%0Alibrary(%22irr%22)%0Aicc(anxiety%2C%20model%20%3D%20%22twoway%22%2C%20type%20%3D%20%22agreement%22%2C%20unit%20%3D%20%22single%22)' frameborder='0'></iframe>


<h3>References:</h3>
<p>
Koo, Terry, and Mae Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” Journal of Chiropractic Medicine 15 (March). doi:10.1016/j.jcm.2016.02.012.<br>
Shrout, P.E., and J.L. Fleiss. 1979. “Intraclass Correlation: Uses in Assessing Rater Reliability.” Psychological Bulletin 86: 420–28.
McGraw KO, Wong SP. Forming inferences about some intraclass correlation coefficients. Psychol Methods. 1996;1:30–46. [Google Scholar]
 </p>


</body>
</html>